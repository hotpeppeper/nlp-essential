{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"finetune-T5-for-summarization.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1-PniDEozU5VyfLyp97zz7wWE20Yy2JUo","authorship_tag":"ABX9TyOz/kImPIZQ2v8DdC3d95Tr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PBdKQKQBdtW4","executionInfo":{"status":"ok","timestamp":1616995890768,"user_tz":-480,"elapsed":3996,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"c6f29fbe-622c-4416-e588-6534906644de"},"source":["!pip install sentencepiece"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\r\u001b[K     |▎                               | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 31.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 23.4MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 19.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 21.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 16.2MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 15.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 15.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 15.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 16.5MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 16.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 16.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 16.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 16.5MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 16.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 16.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 16.5MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FoxkpxoU8eg","executionInfo":{"status":"ok","timestamp":1616995899107,"user_tz":-480,"elapsed":6651,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"7d58acf8-163f-4a5f-af76-c4092abed89f"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n","\u001b[K     |████████████████████████████████| 2.0MB 19.3MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 50.6MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 48.7MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=2f076e0e23ee3e8aeb3866deb3e19e23f458dedc396cc71a7efd0318435e0b4e\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Atv3UaeVdu7","executionInfo":{"status":"ok","timestamp":1616995901374,"user_tz":-480,"elapsed":840,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"86d673b1-6b32-4a27-b0b5-966eee744b44"},"source":["! ls drive/MyDrive/datasets"],"execution_count":3,"outputs":[{"output_type":"stream","text":["bert_multilabels.csv  news_summary_more.csv  sample_submission.csv\n","news_summary.csv      roberta_senti.tsv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vpgEFM1MVsif","executionInfo":{"status":"ok","timestamp":1616995907022,"user_tz":-480,"elapsed":4767,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}}},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-_jOLS8WXID","executionInfo":{"status":"ok","timestamp":1616995908603,"user_tz":-480,"elapsed":632,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"4ee7b411-4222-4ad6-f795-437731acf1c5"},"source":["!nvidia-smi"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mon Mar 29 05:31:48 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   57C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wL5X3lD8WaWc","executionInfo":{"status":"ok","timestamp":1616995910077,"user_tz":-480,"elapsed":626,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"9775a5ce-1f53-44d0-ec8e-4bbba7d4d847"},"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","device"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"nyHxaS1qW_Ot","executionInfo":{"status":"ok","timestamp":1616996311082,"user_tz":-480,"elapsed":807,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}}},"source":["class Customdataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, source_len, target_len):\n","        self.tokenizer = tokenizer\n","        print(self.tokenizer)\n","        self.source_len = source_len\n","        self.target_len = target_len\n","        self.data = dataframe\n","        self.text = self.data.text\n","        self.ctext = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        source = self.tokenizer.batch_encode_plus([ctext], truncation=True, max_length=512,\n","                                                  pad_to_max_length=True, return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([text], truncation=True, max_length=512,\n","                                                  pad_to_max_length=True, return_tensors='pt')\n","        \n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long),\n","            'source_mask': source_mask.to(dtype=torch.long),\n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_mask': target_mask.to(dtype=torch.long)\n","        }"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"6s-_zk4rXxdZ","executionInfo":{"status":"ok","timestamp":1616996879097,"user_tz":-480,"elapsed":779,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}}},"source":["def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _, data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device)\n","        mask = data['source_mask'].to(device)\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=lm_labels)\n","        loss = outputs[0]\n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZnbnocjfA-T","executionInfo":{"status":"ok","timestamp":1616999226091,"user_tz":-480,"elapsed":775,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}}},"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device)\n","            ids = data['source_ids'].to(device)\n","            mask = data['source_mask'].to(device)\n","\n","            generated_ids = model.generate(\n","                input_ids=ids,\n","                attention_mask=mask,\n","                max_length=150,\n","                num_beams=2,\n","                repetition_penalty=2.5,\n","                length_penalty=1.0,\n","                early_stopping=True\n","            )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n","\n","            if _%100==0:\n","                print(f'Completed {_}')\n","            \n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"lCUdfzzagoAv","executionInfo":{"status":"ok","timestamp":1616996881935,"user_tz":-480,"elapsed":517,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}}},"source":["TRAIN_BATCH_SIZE = 2\n","VALID_BATCH_SIZE = 2 \n","TRAIN_EPOCHS = 2\n","VAL_EPOCHS = 1\n","LEARNING_RATE = 1e-4 \n","SEED = 42 \n","MAX_LEN = 512\n","SUMMARY_LEN = 150"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"50SoO1XNhC_k","executionInfo":{"status":"ok","timestamp":1616996883334,"user_tz":-480,"elapsed":802,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}}},"source":["torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFluyHkChOcp","executionInfo":{"status":"ok","timestamp":1616996884746,"user_tz":-480,"elapsed":887,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"e123ad4b-c3a9-4c12-8225-5edc0cb1c7f0"},"source":["tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","type(tokenizer)"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["transformers.models.t5.tokenization_t5.T5Tokenizer"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":493},"id":"Gr-rofNnhWpT","executionInfo":{"status":"ok","timestamp":1616996886157,"user_tz":-480,"elapsed":963,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"7a026876-a402-49cd-916e-4dce4ad9f3bf"},"source":["df = pd.read_csv('drive/MyDrive/datasets/news_summary.csv', encoding='latin-1')\n","df.head()"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>date</th>\n","      <th>headlines</th>\n","      <th>read_more</th>\n","      <th>text</th>\n","      <th>ctext</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Chhavi Tyagi</td>\n","      <td>03 Aug 2017,Thursday</td>\n","      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n","      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n","      <td>The Administration of Union Territory Daman an...</td>\n","      <td>The Daman and Diu administration on Wednesday ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Daisy Mowke</td>\n","      <td>03 Aug 2017,Thursday</td>\n","      <td>Malaika slams user who trolled her for 'divorc...</td>\n","      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n","      <td>Malaika Arora slammed an Instagram user who tr...</td>\n","      <td>From her special numbers to TV?appearances, Bo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Arshiya Chopra</td>\n","      <td>03 Aug 2017,Thursday</td>\n","      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n","      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n","      <td>The Indira Gandhi Institute of Medical Science...</td>\n","      <td>The Indira Gandhi Institute of Medical Science...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sumedha Sehra</td>\n","      <td>03 Aug 2017,Thursday</td>\n","      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n","      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n","      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n","      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Aarushi Maheshwari</td>\n","      <td>03 Aug 2017,Thursday</td>\n","      <td>Hotel staff to get training to spot signs of s...</td>\n","      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n","      <td>Hotels in Maharashtra will train their staff t...</td>\n","      <td>Hotels in Mumbai and other Indian cities are t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               author  ...                                              ctext\n","0        Chhavi Tyagi  ...  The Daman and Diu administration on Wednesday ...\n","1         Daisy Mowke  ...  From her special numbers to TV?appearances, Bo...\n","2      Arshiya Chopra  ...  The Indira Gandhi Institute of Medical Science...\n","3       Sumedha Sehra  ...  Lashkar-e-Taiba's Kashmir commander Abu Dujana...\n","4  Aarushi Maheshwari  ...  Hotels in Mumbai and other Indian cities are t...\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"xfM5rJxHhsOs","executionInfo":{"status":"ok","timestamp":1616996887761,"user_tz":-480,"elapsed":832,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"25991672-1868-4da8-cf22-f466da925a52"},"source":["df = df[['text', 'ctext']]\n","df.head()"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>ctext</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Administration of Union Territory Daman an...</td>\n","      <td>The Daman and Diu administration on Wednesday ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Malaika Arora slammed an Instagram user who tr...</td>\n","      <td>From her special numbers to TV?appearances, Bo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The Indira Gandhi Institute of Medical Science...</td>\n","      <td>The Indira Gandhi Institute of Medical Science...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n","      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Hotels in Maharashtra will train their staff t...</td>\n","      <td>Hotels in Mumbai and other Indian cities are t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text                                              ctext\n","0  The Administration of Union Territory Daman an...  The Daman and Diu administration on Wednesday ...\n","1  Malaika Arora slammed an Instagram user who tr...  From her special numbers to TV?appearances, Bo...\n","2  The Indira Gandhi Institute of Medical Science...  The Indira Gandhi Institute of Medical Science...\n","3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  Lashkar-e-Taiba's Kashmir commander Abu Dujana...\n","4  Hotels in Maharashtra will train their staff t...  Hotels in Mumbai and other Indian cities are t..."]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"GQPJeKcDjFBn","executionInfo":{"status":"ok","timestamp":1616996888817,"user_tz":-480,"elapsed":470,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"a0bdf296-15cc-4171-d46f-9158aa574003"},"source":["df.ctext = 'summarize: ' + df.ctext\n","df.head()"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>ctext</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Administration of Union Territory Daman an...</td>\n","      <td>summarize: The Daman and Diu administration on...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Malaika Arora slammed an Instagram user who tr...</td>\n","      <td>summarize: From her special numbers to TV?appe...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The Indira Gandhi Institute of Medical Science...</td>\n","      <td>summarize: The Indira Gandhi Institute of Medi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n","      <td>summarize: Lashkar-e-Taiba's Kashmir commander...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Hotels in Maharashtra will train their staff t...</td>\n","      <td>summarize: Hotels in Mumbai and other Indian c...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text                                              ctext\n","0  The Administration of Union Territory Daman an...  summarize: The Daman and Diu administration on...\n","1  Malaika Arora slammed an Instagram user who tr...  summarize: From her special numbers to TV?appe...\n","2  The Indira Gandhi Institute of Medical Science...  summarize: The Indira Gandhi Institute of Medi...\n","3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  summarize: Lashkar-e-Taiba's Kashmir commander...\n","4  Hotels in Maharashtra will train their staff t...  summarize: Hotels in Mumbai and other Indian c..."]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xctPEef2jSRu","executionInfo":{"status":"ok","timestamp":1616996890140,"user_tz":-480,"elapsed":551,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"c212f281-4e90-4241-ab02-987de42f8ddc"},"source":["train_size = 0.8\n","train_dataset = df.sample(frac=train_size, random_state=SEED)\n","val_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(val_dataset.shape))"],"execution_count":46,"outputs":[{"output_type":"stream","text":["FULL Dataset: (4514, 2)\n","TRAIN Dataset: (3611, 2)\n","TEST Dataset: (903, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7FSzXJRDkQEC","executionInfo":{"status":"ok","timestamp":1616996891357,"user_tz":-480,"elapsed":553,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"33ada987-2c97-492f-c416-8d5e22f4deac"},"source":["training_set = Customdataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n","val_set = Customdataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n","\n","train_params = {\n","        'batch_size': TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","val_params = {\n","    'batch_size': VALID_BATCH_SIZE,\n","    'shuffle': False,\n","    'num_workers': 0\n","    }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","val_loader = DataLoader(val_set, **val_params)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["PreTrainedTokenizer(name_or_path='t5-base', vocab_size=32100, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})\n","PreTrainedTokenizer(name_or_path='t5-base', vocab_size=32100, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yfvIb_z_k-JF","executionInfo":{"status":"ok","timestamp":1616996899981,"user_tz":-480,"elapsed":7930,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"f288589c-945a-49ad-b31a-4cbbef9a3288"},"source":["model = T5ForConditionalGeneration.from_pretrained('t5-base')\n","model.to(device)"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"PI28_MLwlGxK","executionInfo":{"status":"ok","timestamp":1616996901062,"user_tz":-480,"elapsed":515,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}}},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kl_xc5H0liv9","executionInfo":{"status":"ok","timestamp":1616999168722,"user_tz":-480,"elapsed":2266695,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"17d7bcec-c89e-443b-db05-1869a848b193"},"source":["for epoch in range(TRAIN_EPOCHS):\n","    train(epoch, tokenizer, model, device, training_loader, optimizer)"],"execution_count":50,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0, Loss:  7.753025054931641\n","Epoch: 0, Loss:  2.0994794368743896\n","Epoch: 0, Loss:  1.4805290699005127\n","Epoch: 0, Loss:  1.9056164026260376\n","Epoch: 1, Loss:  1.061751127243042\n","Epoch: 1, Loss:  1.2488007545471191\n","Epoch: 1, Loss:  0.9905338883399963\n","Epoch: 1, Loss:  1.3061045408248901\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uwmjxys6lu2I"},"source":["for epoch in range(VAL_EPOCHS):\n","    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"1gjySPyQqVyf","executionInfo":{"status":"ok","timestamp":1617000258823,"user_tz":-480,"elapsed":900,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}},"outputId":"959b38f3-4534-4284-98f3-46338b9840bf"},"source":["final_df.head(10)"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Generated Text</th>\n","      <th>Actual Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hotels in Mumbai and other Indian cities are t...</td>\n","      <td>Hotels in Maharashtra will train their staff t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Congress has opened a 'State Bank of Tomato' i...</td>\n","      <td>The Congress party has opened a bank called 'S...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>24-year-old Indian athlete Tanveer Hussain has...</td>\n","      <td>Tanveer Hussain, a 24-year-old Indian athlete ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>remains of a German hiker who disappeared whil...</td>\n","      <td>The remains of a German hiker, who disappeared...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GP Manish Shah, who practised in east London, ...</td>\n","      <td>A UK-based doctor, Manish Shah, has been charg...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>a recent study has found a substantial presenc...</td>\n","      <td>Substantial presence of PM1, reportedly the mo...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>US President Donald Trump has told him he woul...</td>\n","      <td>Republican Senator Lindsey Graham has said tha...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Daman and Diu administration on Wednesday with...</td>\n","      <td>The Administration of Union Territory Daman an...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Naseeruddin Shah has said that the Censor Boar...</td>\n","      <td>Actor Naseeruddin Shah, while talking about th...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>a session court in Mumbai convicted 15 Somali ...</td>\n","      <td>A Mumbai court has convicted 15 Somali pirates...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      Generated Text                                        Actual Text\n","0  hotels in Mumbai and other Indian cities are t...  Hotels in Maharashtra will train their staff t...\n","1  Congress has opened a 'State Bank of Tomato' i...  The Congress party has opened a bank called 'S...\n","2  24-year-old Indian athlete Tanveer Hussain has...  Tanveer Hussain, a 24-year-old Indian athlete ...\n","3  remains of a German hiker who disappeared whil...  The remains of a German hiker, who disappeared...\n","4  GP Manish Shah, who practised in east London, ...  A UK-based doctor, Manish Shah, has been charg...\n","5  a recent study has found a substantial presenc...  Substantial presence of PM1, reportedly the mo...\n","6  US President Donald Trump has told him he woul...  Republican Senator Lindsey Graham has said tha...\n","7  Daman and Diu administration on Wednesday with...  The Administration of Union Territory Daman an...\n","8  Naseeruddin Shah has said that the Censor Boar...  Actor Naseeruddin Shah, while talking about th...\n","9  a session court in Mumbai convicted 15 Somali ...  A Mumbai court has convicted 15 Somali pirates..."]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"F3rbKgTpuPuA","executionInfo":{"status":"ok","timestamp":1617000299709,"user_tz":-480,"elapsed":822,"user":{"displayName":"dan lu","photoUrl":"","userId":"10648781224105514833"}}},"source":["final_df.to_csv('gen.csv')"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"okJT2BpzunlD"},"source":[""],"execution_count":null,"outputs":[]}]}